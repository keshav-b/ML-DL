{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkNLP_Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keshav-b/Spark-NLP/blob/master/SparkNLP_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akcer5clBm9u",
        "colab_type": "text"
      },
      "source": [
        "# Create a DataFrame and visualize it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiW-nQ7DBxKM",
        "colab_type": "code",
        "outputId": "9812fa21-2674-48b9-c23d-68a75601965e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "! pip install pyspark spark-nlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 64kB/s \n",
            "\u001b[?25hCollecting spark-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/dc/a3d36b2dc6c8df87098e39234656356e9035bd493945fcbbdca279294cac/spark_nlp-2.3.4-py2.py3-none-any.whl (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.5MB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 67.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=6a4f9fff90bbce9cf75a3e678454ccbd6da2f0aef7c0c160c6c4a4e0b0fb7ba1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, spark-nlp\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4 spark-nlp-2.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o44VKK4xB2GQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sparknlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlzNMle5CTWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starts a session of sparkNLP\n",
        "spark = sparknlp.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu99HgpgCV-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document = DocumentAssembler().setInputCol('text').setOutputCol('document')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqX5XIKZChyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = spark.createDataFrame([['this is the first sentence']]).toDF('text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSa9Vs0pC7NA",
        "colab_type": "code",
        "outputId": "eea1be70-1987-4933-e767-ffc8261ecfc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "data.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|this is the first...|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnTo6JKLC_rp",
        "colab_type": "code",
        "outputId": "4f088253-f88f-4303-9fd6-f1ab4848c20f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# transforming into document\n",
        "doc_data = document.transform(data)\n",
        "doc_data.show(truncate = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------+--------------------------------------------------------------------+\n",
            "|text                      |document                                                            |\n",
            "+--------------------------+--------------------------------------------------------------------+\n",
            "|this is the first sentence|[[document, 0, 25, this is the first sentence, [sentence -> 0], []]]|\n",
            "+--------------------------+--------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbZOu5dSDqyG",
        "colab_type": "text"
      },
      "source": [
        "# Annotators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp5b2ysQEEDn",
        "colab_type": "text"
      },
      "source": [
        "**Tokenizer**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un-2xZedEGVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('tokens')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRiwl1GhEROv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_data = tokenizer.fit(doc_data).transform(doc_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHvNDGYhEVll",
        "colab_type": "code",
        "outputId": "d93a8aac-6f4e-45e8-e291-4aa9d7196724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "tokenized_data.show(truncate= False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------+--------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                      |document                                                            |tokens                                                                                                                                                                                                                   |\n",
            "+--------------------------+--------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|this is the first sentence|[[document, 0, 25, this is the first sentence, [sentence -> 0], []]]|[[token, 0, 3, this, [sentence -> 0], []], [token, 5, 6, is, [sentence -> 0], []], [token, 8, 10, the, [sentence -> 0], []], [token, 12, 16, first, [sentence -> 0], []], [token, 18, 25, sentence, [sentence -> 0], []]]|\n",
            "+--------------------------+--------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOHkpeEcEbHo",
        "colab_type": "code",
        "outputId": "22adcb44-4e43-4907-b3a5-92f1c384fb70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# To see the tokens alone\n",
        "tokenized_data.select('tokens').show(truncate= False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|tokens                                                                                                                                                                                                                   |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[[token, 0, 3, this, [sentence -> 0], []], [token, 5, 6, is, [sentence -> 0], []], [token, 8, 10, the, [sentence -> 0], []], [token, 12, 16, first, [sentence -> 0], []], [token, 18, 25, sentence, [sentence -> 0], []]]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VR7wM5KEjkw",
        "colab_type": "code",
        "outputId": "4e60c8b7-317f-4e37-9c04-83d477606461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# To view a better version of the tokenized text\n",
        "tokenized_data.select('tokens.result').show(truncate= False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------+\n",
            "|result                          |\n",
            "+--------------------------------+\n",
            "|[this, is, the, first, sentence]|\n",
            "+--------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THFZgQOgFi8j",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSGpctqkFkTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtgqBo7EFyci",
        "colab_type": "text"
      },
      "source": [
        "here our pipeline consists of tranforming the raw_text into a document and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXCC1CrpFonW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline().setStages([document, tokenizer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR8sMNABGJKA",
        "colab_type": "text"
      },
      "source": [
        "Even though there is nothing to train here, we  can convert a pipeline into a pipeline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaUeczh2F8f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = pipeline.fit(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV6T14uFGAax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.transform(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-c_RfdWGC7M",
        "colab_type": "code",
        "outputId": "238f02ae-8ae9-4b67-a1f6-1c71257045f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|                text|            document|              tokens|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|this is the first...|[[document, 0, 25...|[[token, 0, 3, th...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
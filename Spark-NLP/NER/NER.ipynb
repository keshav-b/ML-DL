{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising a spark Session\n",
    "ss=sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of the CoNLL class\n",
    "conll = CoNLL(explodeSentences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataset in CoNLL format\n",
    "data = conll.readDataset(ss,'''INSERT PATH TO CONLL FILE''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instance of WordEmbeddings class; Input: document, token; Output: embeddings\n",
    "embeddings = WordEmbeddings().setInputCols(\"document\",\"token\").setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring with the right Embeddings file(Text format)\n",
    "embeddings.setStoragePath('''INSERT PATH TO EMBEDDINGS FILE''',ReadAs.TEXT) \\\n",
    "    .setDimension(100) \\\n",
    "    .setStorageRef(\"sample-conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instance of NerDLApproach class; Input: sentence, token, embeddings; Output: ner\n",
    "ner_dl = NerDLApproach().setInputCols(['sentence','token','embeddings']) \\\n",
    "    .setOutputCol('ner').setGraphFolder('''INSERT PATH TO FOLDER CONTAINING THE GRAPH''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring with the NerDL\n",
    "ner_dl.setMaxEpochs(1).setValidationSplit(0.25).setEnableOutputLogs(True).setIncludeConfidence(True) \\\n",
    "    .setEvaluationLogExtended(True).setLabelColumn('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a training pipeline\n",
    "training_pipeline = Pipeline().setStages([embeddings, ner_dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ~_~\n",
    "trained_pipeline = training_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_pipeline.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a document Assemler: annotating the data, i.e, labeling it\n",
    "document = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sentence Detector: detects sentences.\n",
    "sentence = SentenceDetector().setInputCols(['document']).setOutputCol('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tokenizer: breaks the words into tokens\n",
    "token = Tokenizer().setInputCols(['sentence']).setOutputCol('token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_converter = NerConverter().setInputCols('sentence','token','ner').setOutputCol('ner_chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a prediction pipeline\n",
    "prediction_pipeline = Pipeline(stages = [document, sentence, token, trained_pipeline, ner_converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction :)\n",
    "prediction_data= ss.createDataFrame([[\"John was the Commissioner of Police, India.\"]]).toDF(\"text\")\n",
    "prediction_data.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = prediction_pipeline.fit(prediction_data)\n",
    "pred = prediction_model.transform(prediction_data)\n",
    "\n",
    "#.select(\"token.result\",\"entiry.result\")\n",
    "#.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.select(\"ner_chunk\",\"ner.result\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.write().overwrite().save(\"./prediction_dl_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for using the predicted model\n",
    "'''\n",
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "\n",
    "loaded_prediction_model = PipelineModel.read().load(\"./prediction_dl_model\")\n",
    "\n",
    "\n",
    "loaded_prediction_model.transform(prediction_data).show(5)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [c for c in 'abcdefghijklmnopqrstuvwxyz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {n: i for i, n in enumerate(characters)}\n",
    "number_dict = {i: w for i, w in enumerate(characters)}\n",
    "n_class = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"kind\", \"yale\", \"make\", \"spin\", \"tale\", \"wire\", \"book\", \"case\", \"love\", \"like\", \"bore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(data):\n",
    "    input_batch, target_batch = [], []\n",
    "\n",
    "    for seq in data:\n",
    "        input = [word_dict[n] for n in seq[:-1]]\n",
    "        target = word_dict[seq[-1]] \n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return Variable(torch.Tensor(input_batch)), Variable(torch.LongTensor(target_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "\n",
    "    def forward(self, X):\n",
    "        input = X.transpose(0, 1) \n",
    "\n",
    "        # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        hidden_state = Variable(torch.zeros(1, len(X), n_hidden))   \n",
    "        \n",
    "        # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        cell_state = Variable(torch.zeros(1, len(X), n_hidden))     \n",
    "\n",
    "        outputs, (h_n, c_n) = self.lstm(input, (hidden_state, cell_state))\n",
    "        outputs = outputs[-1]  \n",
    "        model = torch.mm(outputs, self.W) + self.b  \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = make_batch(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 ===> loss: 0.024077434092760086\n",
      "epoch: 200 ===> loss: 0.0050083971582353115\n",
      "epoch: 300 ===> loss: 0.002119732555001974\n",
      "epoch: 400 ===> loss: 0.0011644053738564253\n",
      "epoch: 500 ===> loss: 0.0007351472740992904\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for epoch in range(1, epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch: {epoch} ===> loss: {loss}\")\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = []\n",
    "for word in test:\n",
    "    f_in.append(word[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are', 'bat', 'bir', 'bes', 'cal', 'cod', 'dow']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = []\n",
    "for index in predict:\n",
    "    f_out.append(number_dict[index.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kin  ==>  d\n",
      "yal  ==>  e\n",
      "mak  ==>  e\n",
      "spi  ==>  n\n",
      "tal  ==>  e\n",
      "wir  ==>  e\n",
      "boo  ==>  k\n",
      "cas  ==>  e\n",
      "lov  ==>  e\n",
      "lik  ==>  e\n",
      "bor  ==>  e\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(f_in)):\n",
    "    print(f_in[i],\" ==> \", f_out[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
